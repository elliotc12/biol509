{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3\n",
    "## Data retrieval, preprocessing, and normalization for ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Outline\n",
    "  \n",
    "* Where do data come from? Data retreival.\n",
    "* Ideal datasets and data types\n",
    "* Common wrangling needs and implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where did you get your data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Generated in-house (stored as CSVs, TSVs, SQL, proprietary, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Collaborators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Public sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scripting data retrieval improves reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to:\n",
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('brca_protein_expression.tsv.gz', <http.client.HTTPMessage at 0x10dde5128>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading a data file from a remote repository\n",
    "import urllib\n",
    "\n",
    "URL = \"https://dcc.icgc.org/api/v1/download?fn=/release_18/Projects/BRCA-US/protein_expression.BRCA-US.tsv.gz\"\n",
    "FILENAME = \"brca_protein_expression.tsv.gz\"\n",
    "\n",
    "urllib.request.urlretrieve(URL, FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Scraping tools such as Mechanize and BeautifulSoup allow extraction of data from websites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTCTTGACACTGATTGATCTGCCAAAAGGGGAAGAATGAGTCCAGCTAGAATCCAGGACTAACCAGCGGGTGAGCTTCAAGGAACAAAGGGCTTCCGCTGG\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# Retrieving data from a remote web service in JSON format that gets converted to a python structure:\n",
    "def get_genome_sequence_ensembl(chromosome, start, end):\n",
    "    \"\"\" API described here http://rest.ensembl.org/documentation/info/sequence_region\"\"\"\n",
    "    url = 'https://rest.ensembl.org/sequence/region/human/{0}:{1}..{2}:1?content-type=application/json'.format(\n",
    "        chromosome, start, end)\n",
    "    r = requests.get(url, headers={\"Content-Type\": \"application/json\"}, timeout=10.000)\n",
    "    if r.ok:\n",
    "        return r.json()['seq']\n",
    "print(get_genome_sequence_ensembl(7, 200000,200100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Pandas covers most of the data retrieval needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icgc_donor_id</th>\n",
       "      <th>project_code</th>\n",
       "      <th>icgc_specimen_id</th>\n",
       "      <th>icgc_sample_id</th>\n",
       "      <th>submitted_sample_id</th>\n",
       "      <th>analysis_id</th>\n",
       "      <th>antibody_id</th>\n",
       "      <th>gene_name</th>\n",
       "      <th>gene_stable_id</th>\n",
       "      <th>gene_build_version</th>\n",
       "      <th>normalized_expression_level</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>verification_platform</th>\n",
       "      <th>platform</th>\n",
       "      <th>experimental_protocol</th>\n",
       "      <th>raw_data_repository</th>\n",
       "      <th>raw_data_accession</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DO4143</td>\n",
       "      <td>BRCA-US</td>\n",
       "      <td>SP8807</td>\n",
       "      <td>SA11426</td>\n",
       "      <td>TCGA-A1-A0SK-01A-21-A13A-20</td>\n",
       "      <td>10694</td>\n",
       "      <td>PAI-1</td>\n",
       "      <td>SERPINE1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.769954</td>\n",
       "      <td>not tested</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M.D. Anderson Reverse Phase Protein Array Core</td>\n",
       "      <td>MDA_RPPA_Core http://tcga-data.nci.nih.gov/tcg...</td>\n",
       "      <td>TCGA</td>\n",
       "      <td>TCGA-A1-A0SK-01A-21-A13A-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  icgc_donor_id project_code icgc_specimen_id icgc_sample_id  \\\n",
       "0        DO4143      BRCA-US           SP8807        SA11426   \n",
       "\n",
       "           submitted_sample_id  analysis_id antibody_id gene_name  \\\n",
       "0  TCGA-A1-A0SK-01A-21-A13A-20        10694       PAI-1  SERPINE1   \n",
       "\n",
       "   gene_stable_id  gene_build_version  normalized_expression_level  \\\n",
       "0             NaN                 NaN                     1.769954   \n",
       "\n",
       "  verification_status  verification_platform  \\\n",
       "0          not tested                    NaN   \n",
       "\n",
       "                                         platform  \\\n",
       "0  M.D. Anderson Reverse Phase Protein Array Core   \n",
       "\n",
       "                               experimental_protocol raw_data_repository  \\\n",
       "0  MDA_RPPA_Core http://tcga-data.nci.nih.gov/tcg...                TCGA   \n",
       "\n",
       "            raw_data_accession  \n",
       "0  TCGA-A1-A0SK-01A-21-A13A-20  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Let's read with pandas\n",
    "# Note that we do not even need to unzip the file before opening!\n",
    "brca_data = pd.read_table(FILENAME, sep=\"\\t\")\n",
    "brca_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pandas can even retrieve from an SQL database directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to \n",
    "# !pip install sqlalchemy\n",
    "# !pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sqlalchemy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-39e42a526d27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Connect to UCSC genomic database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mysql+pymysql://genome@genome-mysql.cse.ucsc.edu/hg38'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoolclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNullPool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# select 3 SNPs from Chromosome Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM snp147Common WHERE chrom='chrY' LIMIT 3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sqlalchemy'"
     ]
    }
   ],
   "source": [
    "import sqlalchemy as sa\n",
    "# Connect to UCSC genomic database\n",
    "engine = sa.create_engine('mysql+pymysql://genome@genome-mysql.cse.ucsc.edu/hg38', poolclass=sa.pool.NullPool)\n",
    "# select 3 SNPs from Chromosome Y\n",
    "pd.read_sql(\"SELECT * FROM snp147Common WHERE chrom='chrY' LIMIT 3\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Pandas dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dataframes are convenient containers for mixed data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Pandas is *incredibly useful* for data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* sklearn is happy to accept Pandas dataframes as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Pandas is built for exploratory analysis, visualization and stat tests / ML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting boilerplate\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "brca_data['normalized_expression_level'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-processing a dataset: when are ready for ML?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ideally, data are organized as sample-by-feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Data from multiple sources are combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Missing data are handled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Features have been combined and manipulated as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Any data that need to be normalized have been normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Data are of the correct type (e.g. categorical vs continuous, boolean vs int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's have a look at Boston housing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "boston = pd.read_table(\"boston_data.csv\", sep=\",\")\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-processing a dataset: when are ready for ML?\n",
    "* ~~Ideally, data are organized as sample-by-feature~~\n",
    "* Data from multiple sources are combined\n",
    "* Missing data are handled\n",
    "* Features have been combined and manipulated as needed\n",
    "* Any data that need to be normalized have been normalized\n",
    "* Data are of correct type (e.g. categorical vs continuous, boolean vs int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combining data from multiple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "boston_second_floor = pd.read_table(\"boston_second_floor.csv\", sep=\",\")\n",
    "boston_second_floor.head()\n",
    "#boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combining data from multiple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's combine boston and boston second floor\n",
    "boston = pd.merge(boston, boston_second_floor, on=\"Id\")\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's add some additional data\n",
    "boston3 = pd.read_table(\"boston_additional.csv\", sep=\",\")\n",
    "boston3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thus far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "boston.tail(10)\n",
    "#boston.shape\n",
    "#boston.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-processing a dataset: when are ready for ML?\n",
    "* ~~Ideally, data are organized as sample-by-feature~~\n",
    "* ~~Data from multiple sources are combined~~\n",
    "* Missing data are handled\n",
    "* Features have been combined and manipulated as needed\n",
    "* Any data that need to be normalized have been normalized\n",
    "* Data are of correct type (e.g. categorical vs continuous, boolean vs int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Missing data\n",
    "There are a number of ways to handle missing data:\n",
    "\n",
    "* Drop all records with a value missing (simplest, but can lead to bias)\n",
    "* Substitute all missing values with an approximated value (usually depends on data and algorithm)\n",
    "* Add additional feature indicating when a value is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Drop all records with missing data\n",
    "#boston.isnull().tail()\n",
    "#boston.isnull().sum()\n",
    "#boston.isnull().sum().sum()\n",
    "boston.tail()\n",
    "boston.dropna().tail()\n",
    "boston.dropna().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Substitute missing values\n",
    "# boston.fillna(\"Value!\").tail()\n",
    "boston.fillna({\"2ndFlrSF\": \"Value1!\", \"LotFrontage\": \"Value2!\"}).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Substitute missing values with mean\n",
    "# print(boston.mean())\n",
    "#boston.fillna(boston.mean()).tail()\n",
    "#boston.fillna(boston.median()).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Add column indicating missing values\n",
    "# boston[\"2ndFlrSF\"].isnull()\n",
    "#boston[\"missing_second_floor\"] = boston[\"2ndFlrSF\"].isnull()\n",
    "# boston.tail()\n",
    "# boston = boston.fillna(boston.mean())\n",
    "# boston.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to upgrade Scikit-learn (and restart Jupyter kernel afterwards) to use Imputer\n",
    "# !pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation is a general technique for \"guessing\" appropriate missing values\n",
    "# It could be implemented as a complex ML regression algorithm or a simple 'take an average' strategy.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit_transform(boston[[\"LotFrontage\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputer.fit_transform(boston[[\"LotFrontage\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to decide how to treat missing data?\n",
    "* Very data-dependent!\n",
    "* Decisions need to be justified and documented\n",
    "* Implement missing data preprocessing in a reproducible way (python script)\n",
    "* Don't create data from nothing\n",
    "* Iris example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-processing a dataset: when are ready for ML?\n",
    "* ~~Ideally, data are organized as sample-by-feature~~\n",
    "* ~~Data from multiple sources are combined~~\n",
    "* ~~Missing data are handled~~\n",
    "* Features have been combined and manipulated as needed\n",
    "* Any data that need to be normalized have been normalized\n",
    "* Data are of correct type (e.g. categorical vs continuous, boolean vs int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# boston.head()\n",
    "boston[\"total_sf\"] = boston[\"1stFlrSF\"] + boston[\"2ndFlrSF\"]\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "boston.head()\n",
    "boston = boston.replace({\"Abnorml\": \"abnormal\", \"Normal\": \"normal\"})\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-processing a dataset: when are ready for ML?\n",
    "* ~~Ideally, data are organized as sample-by-feature~~\n",
    "* ~~Data from multiple sources are combined~~\n",
    "* ~~Missing data are handled~~\n",
    "* ~~Features have been combined and manipulated as needed~~\n",
    "* Any data that need to be normalized have been normalized\n",
    "* Data are of correct type (e.g. categorical vs continuous, boolean vs int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normalization\n",
    "* What is it?\n",
    "* Why do it? (data sources, feature distributions)\n",
    "* Types?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Normalization\n",
    "\n",
    "Many machine learning algorithms expect features to have similar distributions and scales.\n",
    "\n",
    "A classic example is gradient descent, if features are on different scales some weights will update faster than others because the feature values scale the weight updates.\n",
    "\n",
    "There are two common approaches to normalization:\n",
    "\n",
    "* Z-score standardization\n",
    "* Min-max scaling\n",
    "\n",
    "#### Z-score standardization\n",
    "\n",
    "Z-score standardization rescales values so that they have a mean of zero and a standard deviation of 1. Specifically we perform the following transformation:\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "#### Min-max scaling\n",
    "\n",
    "An alternative is min-max scaling that transforms data into the range of 0 to 1. Specifically:\n",
    "\n",
    "$$x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "Min-max scaling is less commonly used but can be useful for image data and in some neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# a = (boston['1stFlrSF'] - boston['1stFlrSF'].mean()) / boston['1stFlrSF'].std()\n",
    "# boston['1stFlrSF'].hist()\n",
    "# boston.head()\n",
    "## boston.total_sf.hist()\n",
    "from sklearn.preprocessing import scale, StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit_transform(boston[['1stFlrSF']]))\n",
    "#scaled_size = pd.Series(scale(boston.total_sf))\n",
    "#scaled_size.hist()\n",
    "#scaled_size.mean()\n",
    "#scaled_size.std(ddof=0)\n",
    "#boston[\"normalized_total_sf\"] = scaled_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit_transform(boston[['1stFlrSF']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other preprocessing / normalization techniques and thoughts\n",
    "* http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "* http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-processing a dataset: when are ready for ML?\n",
    "* ~~Ideally, data are organized as sample-by-feature~~\n",
    "* ~~Data from multiple sources are combined~~\n",
    "* ~~Missing data are handled~~\n",
    "* ~~Features have been combined and manipulated as needed~~\n",
    "* ~~Any data that need to be normalized have been normalized~~\n",
    "* Data are of correct type (e.g. categorical vs continuous, boolean vs int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boston' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6081f071456e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# boston[\"1stFlrSF\"].mean(skipna=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mboston\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CentralAir_bool\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboston\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CentralAir\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Y\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# boston.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# boston[\"SaleCondition\"].dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'boston' is not defined"
     ]
    }
   ],
   "source": [
    "#boston.head()\n",
    "import numpy as np\n",
    "\n",
    "# boston[\"1stFlrSF\"].mean(skipna=False)\n",
    "boston[\"CentralAir_bool\"] = boston[\"CentralAir\"] == \"Y\"\n",
    "# boston.head()\n",
    "# boston[\"SaleCondition\"].dtype\n",
    "#boston[\"SaleCondition\"].head()\n",
    "boston[\"SaleCondition\"].astype(\"category\").dtype\n",
    "#boston[\"SaleCondition\"] = boston[\"SaleCondition\"].astype(\"category\")\n",
    "#boston[\"SaleCondition\"].dtype\n",
    "boston[\"SaleCondition\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit_transform(['yes', 'yes', 'no', 'no'])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.fit_transform(['yes', 'yes', 'no', 'no', 'maybe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "sparse_matrix = ohe.fit_transform(boston[['SaleCondition', 'CentralAir_bool']])\n",
    "sparse_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example of categorical data conversion to boolean features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame([[0,1,2,3,4,5,6],\n",
    "                  [2,np.nan,7,4,9,1,3],\n",
    "                  [0.1,0.12,0.11,0.15,0.16,0.11,0.14],\n",
    "                  [100,120,np.nan,127,130,121,124],\n",
    "                  ['Green','Red','Blue','Blue','Green','Red','Green']], ).T\n",
    "x.columns = ['A', 'B', 'C', 'D', 'E']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat = x.copy()\n",
    "for val in x['E'].unique():\n",
    "    x_cat['E_{0}'.format(val)] = x_cat['E'] == val\n",
    "x_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another option to have one feature per color is to use Pivot\n",
    "# Note that it will create missing data:\n",
    "x.pivot(index='A', columns='E', values='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-processing a dataset: when are ready for ML?\n",
    "* ~~Ideally, data are organized as sample-by-feature~~\n",
    "* ~~Data from multiple sources are combined~~\n",
    "* ~~Missing data are handled~~\n",
    "* ~~Features have been combined and manipulated as needed~~\n",
    "* ~~Any data that need to be normalized have been normalized~~\n",
    "* ~~Data are of correct type (e.g. categorical vs continuous, boolean vs int)~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other types of data storage\n",
    "* Image\n",
    "* Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image\n",
    "\n",
    "Datasets with images also need to follow samples-by-features format.\n",
    "Features in this case are pixels and their intensities. For black and white images intensities are binary. For grayscale they could be integer or floating point numbers. Color images are usually represented as multiple images - one for each color channel (e.g. red / green / blue).\n",
    "\n",
    "Thus each image is represented as a one dimensional array, which is exactly what's needed for ML applications. To visualize it, however, we need to change its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "dataset = fetch_olivetti_faces()     \n",
    "print(\"Dimensionality samples x features\", dataset.data.shape)\n",
    "\n",
    "# first image - pixel intensities\n",
    "dataset.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping to visualize\n",
    "plt.imshow(dataset.data[0].reshape(64, 64), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of normalization of an image\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "image = dataset.data[0].reshape(64, 64)\n",
    "normalized_image = Binarizer(threshold=0.6).fit_transform(image)\n",
    "plt.imshow(normalized_image, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text\n",
    "\n",
    "Text has also to be transformed to samples-by-features format.\n",
    "In the simplest case each document is a sample and ocurrence of words are its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "emails = fetch_20newsgroups(subset='train', categories=['sci.med'], shuffle=True, random_state=0)\n",
    "print(\"Number of documents\", len(emails.data))\n",
    "print(\"Beginning of the first document\", emails.data[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emails' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f84d39b115d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0memails_in_ML_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memails_in_ML_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'emails' is not defined"
     ]
    }
   ],
   "source": [
    "# For every document we count word ocurrence:\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "emails_in_ML_format = CountVectorizer().fit_transform(emails.data)\n",
    "print(emails_in_ML_format.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now this is how the first document looks like:\n",
    "emails_in_ML_format[0].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "livereveal": {
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
